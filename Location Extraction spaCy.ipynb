{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOCATION EXTRACTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Picture1.png](Picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Loading packages into the notebook](#Loading-required-packages)\n",
    "- [Loading and exploring data](#Loading-and-exploring-data)\n",
    "- [Data preprocessing and cleaning](#Data-cleaning-and-Preprocessing)\n",
    "- [spaCy: Loading and exploring](#spaCy:-Loading-and-exploring)\n",
    "- [Extracting location entities](#Extracting-location-entities)\n",
    "- [Cleaning and combining locations](#Cleaning-and-combining-locations)\n",
    "- [Geocoding with Nominatim](#Geocoding-with-Nominatim)\n",
    "- [Visualising data](#Visualising-data)\n",
    "- [Distance Calculations](#Distance-calculation)\n",
    "- [Result visualisation](#Result-visualisation)\n",
    "- [Assignment](#Assignment)\n",
    "- [Improving results using keywords](#Grammatical-Filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data \n",
    "df = pd.read_csv('california_tweets.csv')\n",
    "\n",
    "# shape of the data (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the first (number) rows within the dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Our data is made up of 2000 rows and 5 columns. The text column contains tweet posts sent from different users. The data has been filtered to return only tweets with longitude and latitude values, which will be used later on to verify the accuracy of the location extraction.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the noise in the tweet texts, we clean up our tweets before applying NLP. For this exercise we:\n",
    "\n",
    "- Return only English tweets \n",
    "- Remove special characters\n",
    "- Replace @ with at \n",
    "- Remove resulting empty cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only english tweets \n",
    "df['text_en'] = df.text\n",
    "is_english = df.src_lang == 'en'\n",
    "df.loc[is_english, 'text_en'] = df.loc[is_english, 'text']\n",
    "df = df.loc[is_english]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "def preprocess_tweets(tweets, remove_tokens = ('\\n', '\\r', '\\t', 'RT', r'[^\\x00-\\x7f]'),\n",
    "                meta_information_indicators = ('https:', 'http:', 'www.', '//t.co'),\n",
    "                allowed_punctuation = (',', '.', '.', '!', '?', ' ', ':', '-', ';','@')):\n",
    "    def keep_token(token):\n",
    "        return token not in remove_tokens and\\\n",
    "        not any(token.startswith(meta_token) for meta_token in meta_information_indicators)\n",
    "    \n",
    "    clean_tweets = tweets.apply(lambda x: ' '.join(filter(keep_token, x.split(' '))))\n",
    "    \n",
    "    keep_char = lambda t: t.isalnum() or t in allowed_punctuation\n",
    "    return clean_tweets.apply(lambda x: ''.join(filter(keep_char, list(x))))\n",
    "    \n",
    "clean_tweets = preprocess_tweets(df.text_en)\n",
    "df['clean_text'] = clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use np.nan for all missing values\n",
    "df = df.replace('-', np.nan).fillna(np.nan)\n",
    "\n",
    "# remove empty columns\n",
    "df = df.dropna(how='all', axis='columns')\n",
    "\n",
    "# remove rows without text\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "#Replace @ with at for spaCy syntax \n",
    "df.clean_text = df.clean_text.str.replace(\"@\", \"at \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dropping some columns to reduce data size to necessary columns \n",
    "\n",
    "# Make sure to only run this cell once as a duplicate returns error of cells not found\n",
    "df = df.drop(['Unnamed: 0', 'src_lang', 'text_en'], axis = 1 )\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc to define a slice of rows you would like to view from you data\n",
    "\n",
    "#df1.loc[100:162]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "> What other preprocessing routines could be done or what preprocessing steps could be left out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy: Loading and exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**spaCy**](https://spacy.io/) is a NLP tool developed by Explosion to extract entities in text. Unlike most NLP packages that rely on a Gazetteer to extract locations, spaCy uses word embedding to determine the entity class of a word within the sentence syntax. The advantage of this approach is that it is able to return locations even with spelling errors. Disadvantage is possible false positives due to different sentence syntaxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading spaCy packages \n",
    "import spacy\n",
    "from spacy import displacy # Displacy is used to visualise spaCy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the spaCy model \n",
    "nlp =spacy.load('en_core_web_trf') # model trf higher accuracy, bigger model, slower in exercution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "Before extracting locations within the dataset, we can first play around with self-made example sentences to explore how spaCy works. You can rewrite your own sentences and explore what kind of results spaCy would return.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentences \n",
    "\n",
    "doc = nlp(\"\"\"Hello everyone, welcome to the 2021 summer semester Spring School.\n",
    "        The University of Salzburg in Salzburg, Austria presents the spring school\n",
    "         Did you know, Michael Jackson's jacket was valued at 10 billion dollars?\n",
    "        I first have to Google why Google has so many employees.\n",
    "        Zara and I are doing some shopping in Ikea after which we visit the Hellbrun Palace.\n",
    "        I love the taste of New York Pizza especially when I am in New York\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising spaCy entities\n",
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spacy.explain is used to define the entities returned by spaCy\n",
    "spacy.explain(\"FAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    ">Experiment with spaCy writing different sentence structures. Are they any instances where spaCy wrongly detects or omits an entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting location entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get location information \n",
    "def filter_location_entities(entities):\n",
    "    locations = []\n",
    "    for entity in entities:\n",
    "        if entity.label_ == 'GPE':\n",
    "                locations.append(entity)\n",
    "                \n",
    "    return locations\n",
    "\n",
    "def filter_location_entities1(entities):\n",
    "    locations1 = []\n",
    "    for entity in entities:\n",
    "        if entity.label_ == 'FAC':\n",
    "                locations1.append(entity)\n",
    "                      \n",
    "    return locations1\n",
    "\n",
    "def filter_location_entities2(entities):\n",
    "    locations2 = []\n",
    "    for entity in entities:\n",
    "        if entity.label_ == 'ORG':\n",
    "                locations2.append(entity)\n",
    "                      \n",
    "    return locations2\n",
    "\n",
    "def filter_location_entities3(entities):\n",
    "    locations3 = []\n",
    "    for entity in entities:\n",
    "        if entity.label_ == 'LOC':\n",
    "                locations3.append(entity)\n",
    "                      \n",
    "    return locations3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a new column, ner_text, with entities extracted from the 'text' column\n",
    "df['GPE'] = df['clean_text'].astype(str).apply(lambda x: filter_location_entities(nlp(x).ents))\n",
    "df['FAC'] = df['clean_text'].astype(str).apply(lambda x: filter_location_entities1(nlp(x).ents))\n",
    "df['ORG'] = df['clean_text'].astype(str).apply(lambda x: filter_location_entities2(nlp(x).ents))\n",
    "df['LOC'] = df['clean_text'].astype(str).apply(lambda x: filter_location_entities3(nlp(x).ents))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "Time taken in extracting location entities from a data frame increases with the number of rows present in the data frame. It is often advisable to save the file locally within your PC in case the notebook fails, you will not have to rerun the extraction process again.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#Saving dataframe with extracted location entities \n",
    "outfilename = ('df_location_entities1.csv')\n",
    "df.to_csv(outfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and combining locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading data with location entities extracted\n",
    "df = pd.read_csv('df_location_entities1.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data cleaning: Remove square brakets from location entities\n",
    "df['GPE'] =  df['GPE'].apply(lambda x: x.replace('[','').replace(']',''))\n",
    "df['FAC'] =  df['FAC'].apply(lambda x: x.replace('[','').replace(']',''))\n",
    "df['ORG'] =  df['ORG'].apply(lambda x: x.replace('[','').replace(']',''))\n",
    "df['LOC'] =  df['LOC'].apply(lambda x: x.replace('[','').replace(']',''))\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows without location entity extracted\n",
    "# In this case we drop rows where neither of the four location entities have location extracted.\n",
    "\n",
    "index_names = df[(df['GPE']== '') & (df['FAC']== '') & (df['ORG']== '') & (df['LOC']== '')].index\n",
    "df.drop(index_names, inplace = True)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combining locational entities to get finer and more informed place names\n",
    "# Locations are combined only when both columns are not null\n",
    "\n",
    "df['FAC_GPE'] = np.where(((df['FAC'] != '') & (df['GPE'] != '')), df['FAC'].str.cat(df['GPE'], sep = \", \"), '')\n",
    "df['ORG_GPE'] = np.where(((df['ORG'] != '') & (df['GPE'] != '')), df['ORG'].str.cat(df['GPE'], sep = \", \"), '')\n",
    "df['LOC_GPE'] = np.where(((df['LOC'] != '') & (df['GPE'] != '')), df['LOC'].str.cat(df['GPE'], sep = \", \"), '')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "We combine location entities in a manner that resembles actual addresses starting with a finer place name example **Building name: Stein Hotel** to a more course place reference **city name: Salzburg** to an even coarser place name e.g. **Country: Austria**\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Which other location combinations would make sence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocoding with Nominatim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required packages for geocoding with Nominatim\n",
    "import geopandas as gpd \n",
    "import geopy\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial \n",
    "from geopy import distance\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm, tqdm_notebook # progress bar\n",
    "\n",
    "#initiate \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_agent is used to overide restricts of using Nominatim default user_agent.\n",
    "locator = geopy.geocoders.Nominatim(user_agent='mygeocoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to avoid the error of 'Too many requests 429 error'\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "geocode = RateLimiter(locator.geocode, min_delay_seconds=1)\n",
    "\n",
    "# return locations in english \n",
    "geocode = partial(locator.geocode, language = \"en\", timeout = 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "[**Nominatim**](https://nominatim.org/release-docs/latest/) has a limit of 1 request per second which translates to 86 400 requests per day without retries. Depending on the number of retries done on an address, the final returned results on a single day will certainly be lower than 86 400. Furthermore, Nominatim blocks users from sending multiple requests of the same location. \n",
    "\n",
    "To reduce geocoding time and avoid being blocked, we geocode only the unique locations and map results to our dataset. \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count unique locations in column FAC_GPE\n",
    "df['FAC_GPE'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_FAC_GPE = df.groupby('FAC_GPE')['Unnamed: 0'].unique()\n",
    "outfilename = ('unique_FAC_GPE.csv')\n",
    "unique_FAC_GPE.to_csv(outfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('unique_FAC_GPE.csv')\n",
    "df1.loc[2:10] # Extracting a slice of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting locations (raw, latitude, longitude)\n",
    "\n",
    "df1['geocoded_locations'] = df1['FAC_GPE'].progress_apply(geocode)\n",
    "\n",
    "df1['Lat2'] = df1['geocoded_locations'].apply(lambda x: x.latitude if x else None)\n",
    "df1['Lon2'] = df1['geocoded_locations'].apply(lambda x: x.longitude if x else None)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merging unique geocoded locations to full dataframe \n",
    "\n",
    "df2 = pd.merge(df1,df, on='FAC_GPE')\n",
    "df3 =df2[['clean_text','FAC_GPE', 'long', 'lat', 'Lon2', 'Lat2']]\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output file \n",
    "outfilename = ('geocoded_FAC_GPE.csv')\n",
    "df3.to_csv(outfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping geocoded locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FAC_GPE = pd.read_csv('geocoded_FAC_GPE.csv') \n",
    "df_FAC_GPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe with geocoded values returned\n",
    "df_FAC_GPE_cleaned = df_FAC_GPE[df_FAC_GPE['Lon2'].notna()]\n",
    "df_FAC_GPE_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FAC_GPE_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**Exercise**\n",
    ">- More than 50% of the extracted FAC_GPE locations where not geocoded. What can be the reason for Nominatim's failure to geocode these locations?\n",
    ">- HINT: Use another geocoding service for example Google Maps to check the locations availability. Also check the locations on Open Street Map (Nominatim) service on your browser.\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map visualising packages\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBaseMap(default_location=[37.693943, -122.385880], default_zoom_start=12):\n",
    "    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n",
    "    return base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = statistics.mean(df_FAC_GPE_cleaned['Lat2']) \n",
    "x = statistics.mean(df_FAC_GPE_cleaned['Lon2']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "df_FAC_GPE_cleaned['count'] = 1\n",
    "base_map = generateBaseMap([y,x],8)\n",
    "HeatMap(data=df_FAC_GPE_cleaned[['Lat2', 'Lon2', 'count']].groupby(['Lat2', 'Lon2']).sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(base_map)\n",
    "display(base_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute displacements between the GNSS (Ground truth) and the geocoded locations. We use the displacements to measure the precision of the geocoded locations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_calc (row):\n",
    "    start = (row['lat'], row['long'])\n",
    "    stop = (row['Lat2'], row['Lon2'])\n",
    "\n",
    "    return geodesic(start, stop).km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FAC_GPE_cleaned['distance'] = df_FAC_GPE_cleaned.apply (lambda row: distance_calc (row),axis=1)\n",
    "df_FAC_GPE_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising displacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining names of class\n",
    "displacements = [\"1km\", \"5km\", \"10km\", \"Over 10km\"]\n",
    "\n",
    "FAC_GPE_displacements = pd.cut(df_FAC_GPE_cleaned['distance'], [0, 1, 5, 10, 100000.0], labels=displacements) \n",
    "pd.value_counts(FAC_GPE_displacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FAC_GPE_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**Exercise**\n",
    "\n",
    "- Look at the tweets with distance over 10km. What are the reasons for the big distance?\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class is split into two groups. Each group will have 1 unique task, after which results are shared amoungst the groups and an overal task where both groups discuss on the process of the location extraction and geocoding exercise.  \n",
    "\n",
    "> *Use the location extraction annotation notebook for the assignment.*\n",
    ">\n",
    "> *Annotation Manual.pdf contains the labelling instructions for both exercises*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**Group 1:**\n",
    "\n",
    "To get a better understanding of our results, we need to check how well the model performs. [**F-score, F1-score**](https://deepai.org/machine-learning-glossary-and-terms/f-score) is used to evaluate our location extaction model by computing the confusion matrix (False positive, False negative, True positive, True positive) and combining the precision and recall of the model. To compute the F-score we will label our data for the presence or absence of a location and compare the extracted output against the expected output. Essentially we want to reduce the number of false positive and false negative location extractions and increase the number of true positives and true negative location extractions.\n",
    "\n",
    ">- label data for the presence or absence of a location entity.\n",
    ">- Compute F-score of the model as [1] a geometric mean (Basic F-score) and [2] by considering either recall or precision as more important (provide arguments for your considerations). \n",
    ">- Discuss the obtained F-score values and their implications on the study results. \n",
    ">- Are there any instances which struck out when reviewing the results either when a correct location was extracted together with noise (text not part of the location) or when part of a location is extracted for example York instead of New York? What do we do with such instance?\n",
    "****\n",
    "****\n",
    "\n",
    "**Group 2:**\n",
    "\n",
    "For our case study, we want to extract locations where a user is and not a place reference from the past or future. In the prior steps, we have extracted all location mentions within our tweet dataset regardless of the temporal reference. The unfiltered place references might be one of the reasons for obtaining very high displacements between user location and mentioned location. To avoid computing locations of referenced locations we will label our data with 2 classes; user's present location and other. The aim of this assignment is to filter out referenced locations where a user is not thereby reducing the displacement between the users actual location and the predicted location.  \n",
    "\n",
    ">- Label time frame of location reference (Present time / other)\n",
    ">- Compute and compare the displacements between the two groups of labels and discuss the results.  \n",
    ">- To avoid constantly having to manually label the dataset, discuss strategies that can be used to split temporal references?  \n",
    ">- Going through the data, are there any instances where the geocoding service (Nominatim) fails to geocode a correct location or geocodes to a wrong location? What can be done on such instances?\n",
    "\n",
    "****\n",
    "****\n",
    "**Overal:**\n",
    "\n",
    " Share your findings from the individual group exercises. Discuss the presented method used in the notebook to extract and geocode locations:\n",
    ">- Preprocessing routins \n",
    ">- spaCy entity extraction\n",
    ">- entity combinations \n",
    ">- Nominatim geocoding \n",
    "\n",
    "How can this approach be improved \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possibly useful codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining search area\n",
    "# By running this line of code before geocoding we restrict the search area to locations within California\n",
    "# The code then reduces locations like CA from being geocoded to Canada instead of Califonia\n",
    "\n",
    "#geocode = lambda query: locator.geocode(\"%s, California, USA\" % query, timeout = 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammatical Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining keywords for historial mentions \n",
    "\n",
    "\"\"\"past_keywords = [travelled to\", \"Last week\", 'last night', 'last Monday', 'last tuesday', \n",
    "                  'last wednesday', 'last thursday', 'last friday', 'last saturday', 'last month', \n",
    "                 'last year', 'last winter', 'last summer', 'last autumn', 'last days'  \"yesterday\", \n",
    "                 \"I was in \", 'were in', 'I was at', \"I went\", \"was at \", 'was in ', 'were at ', 'went to ',\n",
    "                 \"landed from\", \"passed through\", \"had visited\", 'had gone to', \"flew from\", 'flew in from',\n",
    "                 'back from', \"throwback\", \"past years\", \"miss being in\", \"I miss \", 'years ago ', 'days ago ',\n",
    "                 'months ago ', 'hours ago ', 'time ago ', 'was leaving in ', 'was staying at', 'was leaving at',\n",
    "                 'was staying in', 'makes me miss', 'im from', 'are from ', 'originally from', 'grew up in', 'grew up at ']\n",
    "\"\"\"\n",
    "#past_searched = '|'.join(past_keywords) # for searching keywords within sentence structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filtering the data to return only data with the specific keyword\n",
    "# case = False makes the search case insensitive \n",
    "# na = false means we dont return errors when there are unexpected types in series \n",
    "\n",
    "df_past = df[df[\"clean_text\"].str.contains(past_searched, case = False, na = False)]\n",
    "df_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining keywords for future mentions \n",
    "# We add space before and after 'to' to make the text standalone\n",
    "\n",
    "\"\"\"future_keywords = [\"going to \",'driving to', \"Taking the train to\", \"taking the car to\",\n",
    "                   \"taking the bus on\", \"headin to\",'heading to', 'headed for', \"leave for\",\n",
    "                   \"leaving for\", 'go to', 'travel to', 'trip to','travelling to', 'moving to',\n",
    "                   'relocating to', 'flying to', \"vist \", 'will be going', 'will be at ',\n",
    "                   'will be in ', 'tomorrow', 'next week', 'next days', 'next Monday',\n",
    "                   'next Tuesday', 'next Wednesday', 'next Thursday', 'next Friday', \n",
    "                   'next Saturday', 'next Sunday', 'tonight at', 'join us for', 'on mondays',\n",
    "                   'on tuesdays', 'on wednesdays', 'on Thursdays', 'on fridays', 'on saturdays',\n",
    "                   'on sundays', 'on weekends', 'on weekends', 'weeks from now', 'next stop ',\n",
    "                   'move to ', 'later on ', 'later this ' ]\n",
    "\"\"\"\"\n",
    "#future_searched = '|'.join(future_keywords) # for searching keywords within sentence structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filtering data to return only future mentions \n",
    "\n",
    "#df_future = df[df[\"clean_text\"].str.contains(future_searched, case = False, na = False)]\n",
    "#df_future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to Table of Contents](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
