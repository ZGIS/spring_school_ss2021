{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Spatial Data Analysis of Disaster-Tweets\n",
    "\n",
    "With this notebook, you can analyse tweets that are related to two natural disasters.\n",
    "How are they related? They were posted in the impacted area of the disaster and posted after the disaster occurred.\n",
    "How can we analyse them? First, get an overview of the dataset, such as if the dataset is complete or if there are outliers. Then you can ask for specific questions and try to extract information from the data that helps you to answer them. \n",
    "\n",
    "Datasets are located in '../tweets'.\n",
    "Two Datasets:\n",
    "* Napa Earthquake tweets -> https://en.wikipedia.org/wiki/2014_South_Napa_earthquake\n",
    "* Hurricane Harvey tweets -> https://en.wikipedia.org/wiki/Hurricane_Harvey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO structure notebook. include statistics about the dataset in the beginning and add more field to the notebook\n",
    "# TODO Add more descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../tweets/napa_tweets.csv', sep=',', error_bad_lines=False, index_col=False, warn_bad_lines=False)\n",
    "#df = pd.read_csv('Ressources_and_Results/hurricane_harvey_tweets.csv', sep=',', error_bad_lines=False, index_col=False, warn_bad_lines=False,encoding='utf8', header=0)\n",
    "df.rename(columns={'Unnamed: 0': 'id'}, inplace=True)\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(background_color='white').generate(' '.join(df['tweet_text']))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download nltk Ressources.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">Do this only once!</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing\n",
    "\n",
    "Test multiple pre-processing procedures and observe their impact on the analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "\n",
    "#tweets' text as list\n",
    "tweets_text = df['tweet_text'].tolist()\n",
    "#lowercase\n",
    "tweets_text=[tweet.lower() for tweet in tweets_text]\n",
    "\n",
    "#remove URLs\n",
    "remove_url_regex = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "tweets_text = filter_tweets_before_tokenization(tweets_text, remove_url_regex)\n",
    "\n",
    "#tokenization\n",
    "tweets_text=[nltk.word_tokenize(tweet) for tweet in tweets_text]\n",
    "\n",
    "#remove special characters\n",
    "remove_sc_regex = r'[^A-Za-z ]+'\n",
    "tweets_text = filter_tweets_after_tokenization(tweets_text, remove_sc_regex)\n",
    "\n",
    "# remove short words\n",
    "remove_short_words_regex = r'\\W*\\b\\w{1,3}\\b'\n",
    "tweets_text = filter_tweets_after_tokenization(tweets_text, remove_short_words_regex)\n",
    "\n",
    "# Remove all user names in the tweet text\n",
    "user_names_regex = r\"@\\S+\"\n",
    "tweets_text = filter_tweets_after_tokenization(tweets_text,user_names_regex)\n",
    "\n",
    "#increase keyword frequency by aggregating similar keywords\n",
    "# check the order if preprocessing routine! e.g. stemming would effect the performance of synonym handling\n",
    "#disaster = 'hurrican'\n",
    "#disaster_terms = ['hurricane', 'hurricaneharvey', 'hurricane_harvey', 'flood', 'storm']\n",
    "#tweets_text = synonym_handling(tweets_text, disaster, disaster_terms)\n",
    "\n",
    "#Remove unique words that appear only once in the dataset\n",
    "frequency = getFrequency(tweets_text)\n",
    "min_frequency_words = 2\n",
    "tweets_text = [[token for token in tweet if frequency[token] > min_frequency_words] for tweet in tweets_text]\n",
    "\n",
    "# Remove stop words\n",
    "# You need to download the stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "stoplist = set(stopwords.words('english'))\n",
    "tweets_text = [[word for word in document if word not in stoplist] for document in tweets_text]\n",
    "\n",
    "#Custom Stop word list\n",
    "custom_stopwords_path = r'Ressources_and_Results\\Stopwordlist_English.txt'\n",
    "custom_stop_words = []\n",
    "with open(custom_stopwords_path, 'r') as sw:\n",
    "    custom_stop_words = [line.rstrip('\\r\\n') for line in sw]\n",
    "sw.close()\n",
    "\n",
    "tweets_text = [[word for word in document if word not in set(custom_stop_words)] for document in tweets_text]\n",
    "\n",
    "#Stemming\n",
    "stemmer = PorterStemmer()\n",
    "#stemmer = SnowballStemmer(\"english\")\n",
    "tweets_text = [[stemmer.stem(word) for word in sub_list] for sub_list in tweets_text]\n",
    "\n",
    "#remove empty strings\n",
    "tweets_text = [[word for word in document if word] for document in tweets_text]\n",
    "\n",
    "tweets_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create corpus and dictionary for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = gensim.corpora.Dictionary(tweets_text)\n",
    "corpus = [dict.doc2bow(text) for text in tweets_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics= 10\n",
    "alpha = 0.0001\n",
    "eta= 0.0001\n",
    "passes = 10\n",
    "lda = gensim.models.LdaMulticore(corpus, id2word=dict, num_topics= num_topics, alpha = alpha, eta= eta, passes = passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show top words of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5\n",
    "\n",
    "#show top words of topics\n",
    "for t in range(lda.num_topics):\n",
    "    print('topic {}: '.format(t) + ', '.join([v[0] for v in lda.show_topic(t, top_words)]))\n",
    "\n",
    "#show top words of topics with probabilities  \n",
    "#for t in range(lda.num_topics):\n",
    "#   print('topic {}: '.format(t) + ', '.join([v[0] + \" (\" + str(v[1]) + \")\" for v in lda.show_topic(t, top_words)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise topics and check relation between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, dict)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify disaster-related topic and classify tweets accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_list = list(lda.get_document_topics(corpus))\n",
    "classified_tweets =[max(document, key=lambda x: x[1]) for document in document_topic_list]\n",
    "topics = [top_prob[0] for top_prob in classified_tweets]\n",
    "probabilites = [top_prob[1] for top_prob in classified_tweets]\n",
    "df['topics'] = topics\n",
    "df['probabilities'] = probabilites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check new axes and null values in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.axes)\n",
    "print('Number of Null values: ' + str(df.isnull().sum().sum()))\n",
    "from shapely.geometry import Point\n",
    "df['Coordinates'] = list(zip(df.longitude, df.latitude))\n",
    "df['Coordinates'] = df['Coordinates'].apply(Point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check classified tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_number = 4\n",
    "df.loc[df['topics'] == topic_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check geospatial distribution of disaster-related tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "topic_number = 9\n",
    "gdf = geopandas.GeoDataFrame(df, geometry='Coordinates')\n",
    "print(gdf.head())\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "ax = world[world.continent == 'North America'].plot(\n",
    "    color='white', edgecolor='black')\n",
    "gdf_topic = gdf.loc[gdf['topics'] == topic_number]\n",
    "gdf_topic.plot(ax=ax, color='green',markersize = 0.3)\n",
    "minx, miny, maxx, maxy = gdf_topic.total_bounds\n",
    "ax.set_xlim(minx-1, maxx+1)\n",
    "ax.set_ylim(miny-1, maxy+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information\n",
    "\n",
    "* [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "* [Combining machine-learning topic models and spatiotemporal analysis of social media data for disaster footprint and damage assessment](https://www.tandfonline.com/doi/full/10.1080/15230406.2017.1356242)\n",
    "* [Gensim](https://radimrehurek.com/gensim/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
