{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Spatial Data Analysis of Disaster-Tweets\n",
    "\n",
    "With this notebook, you can analyse tweets that were posted in the same area and on the same day as the Napa earthquake in 2014 (see https://en.wikipedia.org/wiki/2014_South_Napa_earthquake for details).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Create a new markdown cell to answer the following questions:\n",
    "\n",
    "* How many tweets are in the dataset?\n",
    "* What words are the most frequent words in the wordcloud (related to the size of the words)?\n",
    "* Perform topic modelling with and without preprocessing. What could you observe?\n",
    "* Which topics could you identify in the datasets? Can you label some of the topics?\n",
    "* What is the min and max date of the dataset?\n",
    "* How does the time-series of the disaster-related topic and the overall dataset differ?\n",
    "* How does the heatmap of the disaster-related topic and the overall dataset differ?\n",
    "\n",
    "When you have answered the questions, download your notebook as HTML-file and check if it worked.\n",
    "File -> Download as -> HTML\n",
    "\n",
    "Hint: You do not have to program something by yourself, but you have to understand the code (change some variables) and look for the answers to the questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "import gensim\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from folium.plugins import HeatMap\n",
    "import folium\n",
    "import pyLDAvis.gensim\n",
    "import statistics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../tweets/napa_tweets.csv', sep=',', error_bad_lines=False, index_col=False, warn_bad_lines=False)\n",
    "df.rename(columns={'Unnamed: 0': 'id'}, inplace=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white').generate(' '.join(df['tweet_text']))\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download nltk Ressources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform tweets text into usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets' text as list\n",
    "tweets_text = df['tweet_text'].tolist()\n",
    "#lowercase\n",
    "tweets_text=[tweet.lower() for tweet in tweets_text]\n",
    "\n",
    "#remove URLs\n",
    "remove_url_regex = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "tweets_text = filter_tweets_before_tokenization(tweets_text, remove_url_regex)\n",
    "\n",
    "#tokenization\n",
    "tweets_text=[nltk.word_tokenize(tweet) for tweet in tweets_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Test multiple preprocessing procedures and observe their impact on the analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove special characters\n",
    "remove_sc_regex = r'[^A-Za-z ]+'\n",
    "tweets_text = filter_tweets_after_tokenization(tweets_text, remove_sc_regex)\n",
    "\n",
    "# remove short words\n",
    "remove_short_words_regex = r'\\W*\\b\\w{1,3}\\b'\n",
    "tweets_text = filter_tweets_after_tokenization(tweets_text, remove_short_words_regex)\n",
    "\n",
    "# Remove all user names in the tweet text\n",
    "user_names_regex = r\"@\\S+\"\n",
    "tweets_text = filter_tweets_after_tokenization(tweets_text,user_names_regex)\n",
    "\n",
    "#increase keyword frequency by aggregating similar keywords\n",
    "# check the order if preprocessing routine! e.g. stemming would effect the performance of synonym handling\n",
    "#disaster = 'hurrican'\n",
    "#disaster_terms = ['hurricane', 'hurricaneharvey', 'hurricane_harvey', 'flood', 'storm']\n",
    "#tweets_text = synonym_handling(tweets_text, disaster, disaster_terms)\n",
    "\n",
    "#Remove unique words that appear only once in the dataset\n",
    "frequency = getFrequency(tweets_text)\n",
    "min_frequency_words = 2\n",
    "tweets_text = [[token for token in tweet if frequency[token] > min_frequency_words] for tweet in tweets_text]\n",
    "\n",
    "# Remove stop words\n",
    "# You need to download the stopwords\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "stoplist = set(stopwords.words('english'))\n",
    "tweets_text = [[word for word in document if word not in stoplist] for document in tweets_text]\n",
    "\n",
    "#Stemming\n",
    "stemmer = PorterStemmer()\n",
    "#stemmer = SnowballStemmer(\"english\")\n",
    "tweets_text = [[stemmer.stem(word) for word in sub_list] for sub_list in tweets_text]\n",
    "\n",
    "#remove empty strings\n",
    "tweets_text = [[word for word in document if word] for document in tweets_text]\n",
    "\n",
    "tweets_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create corpus and dictionary for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = gensim.corpora.Dictionary(tweets_text)\n",
    "corpus = [dict.doc2bow(text) for text in tweets_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics= 10\n",
    "alpha = 0.0001\n",
    "eta= 0.0001\n",
    "passes = 10\n",
    "lda = gensim.models.LdaMulticore(corpus, id2word=dict, num_topics= num_topics, alpha = alpha, eta= eta, passes = passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show top words of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5\n",
    "\n",
    "#show top words of topics\n",
    "for t in range(lda.num_topics):\n",
    "    print('topic {}: '.format(t+1) + ', '.join([v[0] for v in lda.show_topic(t, top_words)]))\n",
    "\n",
    "#show top words of topics with probabilities  \n",
    "#for t in range(lda.num_topics):\n",
    "#   print('topic {}: '.format(t+1) + ', '.join([v[0] + \" (\" + str(v[1]) + \")\" for v in lda.show_topic(t, top_words)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise topics and check relation between them\n",
    "If the window is not big enough, you can enlarge it with Cell -> Current Outputs -> Toggle Scrolling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, dict, sort_topics=False)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify disaster-related topic and classify tweets accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_list = list(lda.get_document_topics(corpus))\n",
    "classified_tweets =[max(document, key=lambda x: x[1]) for document in document_topic_list]\n",
    "topics = [top_prob[0]+1 for top_prob in classified_tweets]\n",
    "probabilites = [top_prob[1] for top_prob in classified_tweets]\n",
    "df['topics'] = topics\n",
    "df['probabilities'] = probabilites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check classified tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_number = 6\n",
    "df.loc[df['topics'] == topic_number].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(df['time']))\n",
    "print(max(df['time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_number = 8\n",
    "figure, axes = plt.subplots(1, 2,figsize=(20,8))\n",
    "\n",
    "df_sum = df['time'].value_counts().resample('T').sum()\n",
    "ax = df_sum.plot(label='Number of Tweets',ax=axes[0])\n",
    "axes[0].grid()\n",
    "axes[0].set_ylabel(\"#Tweets\")\n",
    "\n",
    "df_topic = df.loc[df['topics'] == topic_number]\n",
    "df_sum = df_topic['time'].value_counts().resample('T').sum()\n",
    "ax = df_sum.plot(label='Number of Tweets',ax=axes[1])\n",
    "axes[1].grid()\n",
    "axes[1].set_ylabel(\"#Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check geospatial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateBaseMap(default_location=[40.693943, -73.985880], default_zoom_start=12):\n",
    "    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n",
    "    return base_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = statistics.mean(df['latitude']) \n",
    "x = statistics.mean(df['longitude']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "\n",
    "df['count'] = 1\n",
    "base_map = generateBaseMap([y,x],8)\n",
    "HeatMap(data=df[['latitude', 'longitude', 'count']].groupby(['latitude', 'longitude']).sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(base_map)\n",
    "display(base_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_numbers = [4,8]\n",
    "base_maps = []\n",
    "for topic_number in topic_numbers:\n",
    "    df_topic = df.loc[df['topics'] == topic_number]\n",
    "    base_map = generateBaseMap([y,x],8)\n",
    "    HeatMap(data=df_topic[['latitude', 'longitude', 'count']].groupby(['latitude', 'longitude']).sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(base_map)\n",
    "    base_maps.append(base_map)\n",
    "\n",
    "htmlmap = HTML('<iframe srcdoc=\"{}\" style=\"float:left; width: {}px; height: {}px; display:inline-block; width: 50%; margin: 0 auto; border: 2px solid black\"></iframe>'\n",
    "           '<iframe srcdoc=\"{}\" style=\"float:right; width: {}px; height: {}px; display:inline-block; width: 50%; margin: 0 auto; border: 2px solid black\"></iframe>'\n",
    "           .format(base_maps[0].get_root().render().replace('\"', '&quot;'),500,500,\n",
    "                   base_maps[1].get_root().render().replace('\"', '&quot;'),500,500))\n",
    "display(htmlmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information\n",
    "\n",
    "* [Latent Dirichlet Allocation](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "* [Combining machine-learning topic models and spatiotemporal analysis of social media data for disaster footprint and damage assessment](https://www.tandfonline.com/doi/full/10.1080/15230406.2017.1356242)\n",
    "* [Gensim](https://radimrehurek.com/gensim/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
